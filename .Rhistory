dataset = dataset[complete.cases(dataset), ]
# foctor for target attribute
dataset$V9 = ifelse(dataset$V9>1,1,0)
# create a list of 80% of the rows in the original dataset for training
validation_index = createDataPartition(dataset$V9, p=0.80, list=FALSE)
# select 20% of the data for validation
validation = dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset = dataset[validation_index,]
# type of data
class(dataset)
# names of attributes
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# Load Data
dataset =
read.csv(
"new.csv",
header = FALSE,
sep=";"
)
View(dataset)
# Delete id and securitt code
dataset = subset(dataset, select = -c(V1, V2))
# Load Data
dataset =
read.csv(
"new.csv",
header = FALSE,
sep=";"
)
# Delete id and securitt code
dataset = subset(dataset, select = -c(V1, V2, V90))
View(dataset)
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
# foctor for target attribute
dataset$V9 = ifelse(dataset$V9>1,1,0)
dataset[da]
# Load Data
dataset =
read.csv(
"new.csv",
header = FALSE,
sep=";"
)
# Delete id and securitt code
dataset = subset(dataset, select = -c(V1, V2, V90))
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
# foctor for target attribute
dataset$V9 = ifelse(dataset$V9>1,1,0)
# create a list of 80% of the rows in the original dataset for training
validation_index = createDataPartition(dataset$V9, p=0.80, list=FALSE)
# select 20% of the data for validation
validation = dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset = dataset[validation_index,]
# type of data
class(dataset)
# names of attributes
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek (dare un'occhiata) at the first 5 rows of the data
head(dataset)
# summarize the class distribution
# Let's now take a look at the number of instances (rows) that belong to each
# class. We can view this as an absolute count and as a percentage.
percentage = prop.table(table(dataset$num)) * 100
# summarize the class distribution
# Let's now take a look at the number of instances (rows) that belong to each
# class. We can view this as an absolute count and as a percentage.
percentage = prop.table(table(dataset$V9)) * 100
cbind(freq=table(dataset$V9), percentage=percentage)
# summarize attribute distributions
# Now we can take a look at a summary of each attribute.
# This includes the mean, the min and max values as well as some
# percentiles (25th, 50th or media and 75th e.g. values at this points if
# we ordered all the values for an attribute).
summary(dataset)
# 3 repeats of 10-fold cross validation
# control <- trainControl(method="repeatedcv", number=10, repeats=3)
control = trainControl(method="cv", number=10)
metric = "Accuracy"
# before reach run to ensure that the evaluation of each algorithm is
# performed using exactly the same data splits.
# It ensures the results are directly comparable.
#
# basically set.seed() function will help to reuse the same set of random
# variables , which we may need in future to again evaluate particular
# task again with same random varibales.
# We just need to declare it before using any random numbers generating function.
#
# SVM
set.seed(7)
fit.svm <- train(num~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# before reach run to ensure that the evaluation of each algorithm is
# performed using exactly the same data splits.
# It ensures the results are directly comparable.
#
# basically set.seed() function will help to reuse the same set of random
# variables , which we may need in future to again evaluate particular
# task again with same random varibales.
# We just need to declare it before using any random numbers generating function.
#
# SVM
set.seed(7)
fit.svm <- train(V9~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Delete id and securitt code
dataset = subset(dataset, select = -c(V1, V2, V90))
# Load Data
dataset =
read.csv(
"new.csv",
header = FALSE,
sep=";"
)
# Delete id and securitt code
dataset = subset(dataset, select = -c(V1, V2, V90))
# factor for target attribute
dataset$V9 = factor(dataset$V9)
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
# foctor for target attribute
dataset$V9 = ifelse(dataset$V9>1,1,0)
# Load Data
dataset =
read.csv(
"new.csv",
header = FALSE,
sep=";"
)
# Delete id and securitt code
dataset = subset(dataset, select = -c(V1, V2, V90))
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
# factor for target attribute
dataset$V9 = ifelse(dataset$V9>1,1,0)
dataset$V9 = factor(dataset$V9)
# create a list of 80% of the rows in the original dataset for training
validation_index = createDataPartition(dataset$V9, p=0.80, list=FALSE)
# select 20% of the data for validation
validation = dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset = dataset[validation_index,]
# type of data
class(dataset)
# names of attributes
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek (dare un'occhiata) at the first 5 rows of the data
head(dataset)
# summarize the class distribution
# Let's now take a look at the number of instances (rows) that belong to each
# class. We can view this as an absolute count and as a percentage.
percentage = prop.table(table(dataset$V9)) * 100
cbind(freq=table(dataset$V9), percentage=percentage)
# summarize attribute distributions
# Now we can take a look at a summary of each attribute.
# This includes the mean, the min and max values as well as some
# percentiles (25th, 50th or media and 75th e.g. values at this points if
# we ordered all the values for an attribute).
summary(dataset)
# 3 repeats of 10-fold cross validation
# control <- trainControl(method="repeatedcv", number=10, repeats=3)
control = trainControl(method="cv", number=10)
metric = "Accuracy"
# before reach run to ensure that the evaluation of each algorithm is
# performed using exactly the same data splits.
# It ensures the results are directly comparable.
#
# basically set.seed() function will help to reuse the same set of random
# variables , which we may need in future to again evaluate particular
# task again with same random varibales.
# We just need to declare it before using any random numbers generating function.
#
# SVM
set.seed(7)
fit.svm <- train(V9~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(V9~., data=dataset, method="rf", metric=metric, trControl=control)
# Neural Network
set.seed(7)
fit.nnet <- train(V9~., data=dataset, method="nnet", metric=metric, trControl=control)
# Naive bayes
set.seed(7)
fit.nb <- train(V9~., data=dataset, method="nb", metric=metric, trControl=control)
# Select Best Model
# We now have 5 models and accuracy estimations for each.
# We need to compare the models to each other and select the most accurate.
# We can report on the accuracy of each model by first creating a list of
# the created models and using the summary function.
#
# We can see the accuracy of each classifier and also other metrics like Kappa
list_models = list(svm=fit.svm, rf=fit.rf, nnet=fit.nnet, nb=fit.nb)
results = resamples(list_models)
summary(results)
# We can also create a plot of the model evaluation results and compare the
# spread and the mean accuracy of each model. There is a population of accuracy
# measures for each algorithm because each algorithm was evaluated 10 times
# (10 fold cross validation).
dotplot(results)
# We get the model with best accurancy
maxAcc = 0
for(item in list_models){
meanAcc = mean(item[["resample"]][["Accuracy"]])
if(meanAcc>maxAcc){
maxAcc=meanAcc
bestModel=item
}
}
# We can see that the most accurate model in this case was SVM
# The results for just the SVM model can be summarized
# summarize Best Model
print(bestModel)
# save the model to disk
saveRDS(bestModel, "./final_model.rds")
# load the model
super_model <- readRDS("./final_model.rds")
print(super_model)
# slip during such as overfitting to the training set or a data leak.
# Both will result in an overly optimistic result.
#
# We can run the model directly on the validation set and summarize the
# results in a confusion matrix.
#
# We can see that the accuracy is 100%.
# It was a small validation dataset (20%), but this result is within
# our expected margin of 97% +/-4% suggesting we may have an accurate
# and a reliably (affidabile) accurate model.
predictions = predict(super_model, validation)
confusionMatrix(predictions, validation$num)
confusionMatrix(predictions, validation$V9)
#16 (fbs)
#19 (restecg)
#32 (thalach)
#38 (exang)
#40 (oldpeak)
#41 (slope)
#44 (ca)
#51 (thal)
#58 (num) (the predicted attribute)
#
dataset =
read.csv(
"14.csv",
#url("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"),
header = FALSE,
na.strings = "?",
col.names = c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
)
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
# factor for target attribute
dataset$num[dataset$num > 0] = 1
View(dataset)
#16 (fbs)
#19 (restecg)
#32 (thalach)
#38 (exang)
#40 (oldpeak)
#41 (slope)
#44 (ca)
#51 (thal)
#58 (num) (the predicted attribute)
#
dataset =
read.csv(
"14.csv",
#url("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"),
header = FALSE,
na.strings = "?",
sep=";",
col.names = c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
)
View(dataset)
#16 (fbs)
#19 (restecg)
#32 (thalach)
#38 (exang)
#40 (oldpeak)
#41 (slope)
#44 (ca)
#51 (thal)
#58 (num) (the predicted attribute)
#
dataset =
read.csv(
"14.csv",
#url("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"),
header = FALSE,
na.strings = "?",
sep=";",
col.names = c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
)
View(dataset)
#16 (fbs)
#19 (restecg)
#32 (thalach)
#38 (exang)
#40 (oldpeak)
#41 (slope)
#44 (ca)
#51 (thal)
#58 (num) (the predicted attribute)
#
dataset =
read.csv(
"14.csv",
#url("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"),
header = FALSE,
na.strings = "?",
sep=";",
col.names = c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
)
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
View(dataset)
#16 (fbs)
#19 (restecg)
#32 (thalach)
#38 (exang)
#40 (oldpeak)
#41 (slope)
#44 (ca)
#51 (thal)
#58 (num) (the predicted attribute)
#
dataset =
read.csv(
"14.csv",
#url("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"),
header = FALSE,
na.strings = "?",
fileEncoding="UTF-8-BOM",
sep=";",
col.names = c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
)
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
View(dataset)
#16 (fbs)
#19 (restecg)
#32 (thalach)
#38 (exang)
#40 (oldpeak)
#41 (slope)
#44 (ca)
#51 (thal)
#58 (num) (the predicted attribute)
#
dataset =
read.csv(
"14.csv",
#url("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"),
header = FALSE,
na.strings = "?",
fileEncoding="UTF-8-BOM",
sep=";",
col.names = c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
)
#16 (fbs)
#19 (restecg)
#32 (thalach)
#38 (exang)
#40 (oldpeak)
#41 (slope)
#44 (ca)
#51 (thal)
#58 (num) (the predicted attribute)
#
dataset =
read.csv(
"14.csv",
#url("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"),
header = FALSE,
na.strings = "?",
fileEncoding="UTF-8-BOM",
sep=";",
col.names = c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
)
# to remove rows with null attribute values
dataset = dataset[complete.cases(dataset), ]
# factor for target attribute
dataset$num[dataset$num > 0] = 1
dataset$num = factor(dataset$num)
# create a list of 80% of the rows in the original dataset for training
validation_index = createDataPartition(dataset$num, p=0.80, list=FALSE)
# select 20% of the data for validation
validation = dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset = dataset[validation_index,]
# type of data
class(dataset)
# names of attributes
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek (dare un'occhiata) at the first 5 rows of the data
head(dataset)
# summarize the class distribution
# Let's now take a look at the number of instances (rows) that belong to each
# class. We can view this as an absolute count and as a percentage.
percentage = prop.table(table(dataset$num)) * 100
cbind(freq=table(dataset$num), percentage=percentage)
# summarize attribute distributions
# Now we can take a look at a summary of each attribute.
# This includes the mean, the min and max values as well as some
# percentiles (25th, 50th or media and 75th e.g. values at this points if
# we ordered all the values for an attribute).
summary(dataset)
# We are going to look at two types of plots:
# 1.Univariate plots to better understand each attribute.
# split input and output
x <- dataset[,1:13]
y <- dataset[,14]
# boxplot for each attribute on one image
par(mfrow=c(1,4))
for(i in 1:4) {
boxplot(x[,i], main=names(dataset)[i])
}
# barplot for class breakdown (generally uninteresting in this
# case because they're even).
plot(y)
# density plots for each attribute by class value
# Like the boxplots, we can see the difference in distribution of each
# attribute by class value. We can also see the Gaussian-like distribution
# (bell curve) of each attribute.
scales = list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
# 3 repeats of 10-fold cross validation
# control <- trainControl(method="repeatedcv", number=10, repeats=3)
control = trainControl(method="cv", number=10)
metric = "Accuracy"
# before reach run to ensure that the evaluation of each algorithm is
# performed using exactly the same data splits.
# It ensures the results are directly comparable.
#
# basically set.seed() function will help to reuse the same set of random
# variables , which we may need in future to again evaluate particular
# task again with same random varibales.
# We just need to declare it before using any random numbers generating function.
#
# SVM
set.seed(7)
fit.svm <- train(num~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(num~., data=dataset, method="rf", metric=metric, trControl=control)
# Neural Network
set.seed(7)
fit.nnet <- train(num~., data=dataset, method="nnet", metric=metric, trControl=control)
# Naive bayes
set.seed(7)
fit.nb <- train(num~., data=dataset, method="nb", metric=metric, trControl=control)
# Select Best Model
# We now have 5 models and accuracy estimations for each.
# We need to compare the models to each other and select the most accurate.
# We can report on the accuracy of each model by first creating a list of
# the created models and using the summary function.
#
# We can see the accuracy of each classifier and also other metrics like Kappa
list_models = list(svm=fit.svm, rf=fit.rf, nnet=fit.nnet, nb=fit.nb)
results = resamples(list_models)
summary(results)
# We can also create a plot of the model evaluation results and compare the
# spread and the mean accuracy of each model. There is a population of accuracy
# measures for each algorithm because each algorithm was evaluated 10 times
# (10 fold cross validation).
dotplot(results)
# We get the model with best accurancy
maxAcc = 0
for(item in list_models){
meanAcc = mean(item[["resample"]][["Accuracy"]])
if(meanAcc>maxAcc){
maxAcc=meanAcc
bestModel=item
}
}
# We can see that the most accurate model in this case was SVM
# The results for just the SVM model can be summarized
# summarize Best Model
print(bestModel)
# save the model to disk
saveRDS(bestModel, "./final_model.rds")
# load the model
super_model <- readRDS("./final_model.rds")
print(super_model)
# slip during such as overfitting to the training set or a data leak.
# Both will result in an overly optimistic result.
#
# We can run the model directly on the validation set and summarize the
# results in a confusion matrix.
#
# We can see that the accuracy is 100%.
# It was a small validation dataset (20%), but this result is within
# our expected margin of 97% +/-4% suggesting we may have an accurate
# and a reliably (affidabile) accurate model.
predictions = predict(super_model, validation)
confusionMatrix(predictions, validation$num)
